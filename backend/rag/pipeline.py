"""
pipeline.py — Full RAG query orchestration.

This module is the central coordinator for all query processing. It wires
together the deterministic router, retriever, prompt assembler, Groq LLM
client, output evaluator, and structured logger.

Query pipeline steps (per Section 1 — Data Flow):
  1. Deterministic Router: classify query as 'simple' or 'complex'.
  2. Select Groq model and max_tokens based on classification.
  3. Embed query and retrieve top-k chunks from FAISS.
  4. Assemble prompt using retrieved chunks.
  5. Call Groq API (streaming or non-streaming).
  6. Run Output Evaluator (3 checks).
  7. Write structured log entry.
  8. Return result dict (for non-streaming) or async generator (for streaming).

Streaming SSE event format (when stream=True):
  {"event": "token", "data": '{"token": "..."}'}     — emitted per token
  {"event": "done",  "data": '<metadata JSON>'}       — emitted once at end
  {"event": "error", "data": '<error JSON>'}          — emitted on failure

The route handler is responsible for wrapping the async generator in an
EventSourceResponse (sse-starlette). It must NOT await run_sse_pipeline().
"""
import asyncio
import json
import logging
import time
from typing import Any, AsyncGenerator, Dict, List

from backend.config import settings
from backend.evaluator.output_evaluator import evaluate_output
from backend.llm.groq_client import groq_client
from backend.logging_.structured_logger import log_query
from backend.rag.prompt_assembler import assemble_prompt
from backend.rag.retriever import RetrievedChunk, retrieve
from backend.router.deterministic_router import classify_query
from backend.utils.text_sanitizer import sanitize_input

logger = logging.getLogger(__name__)

# Model token caps per classification (Section 4.1 — immutable)
_MAX_TOKENS: Dict[str, int] = {
    "simple": 512,
    "complex": 1024,
}


def _sources_from_chunks(chunks: List[RetrievedChunk]) -> List[Dict[str, Any]]:
    """Build the sources list payload from retrieved chunks."""
    return [
        {
            "source_file": c.source_file,
            "page_number": c.page_number,
            "score": c.score,
        }
        for c in chunks
    ]


def _debug_payload(
    classification: str,
    model: str,
    tokens_input: int,
    tokens_output: int,
    latency_ms: float,
    retrieval_count: int,
    evaluator_flags: List[str],
) -> Dict[str, Any]:
    """Build the debug metadata dict for a response payload."""
    return {
        "classification": classification,
        "model_used": model,
        "tokens_input": tokens_input,
        "tokens_output": tokens_output,
        "latency_ms": round(latency_ms, 2),
        "retrieval_count": retrieval_count,
        "evaluator_flags": evaluator_flags,
    }


# ---------------------------------------------------------------------------
# Non-streaming pipeline
# ---------------------------------------------------------------------------


async def run_query_pipeline(
    query: str,
    request_id: str,
) -> Dict[str, Any]:
    """
    Run the full RAG query pipeline in non-streaming mode.

    Args:
        query: Raw user query string (pre-validated by Pydantic, max 2000 chars).
        request_id: UUID4 generated by the route handler for this request.

    Returns:
        Dict conforming to the QueryResponse schema (Section 6.1).

    Raises:
        ConnectionError: If Groq API is unreachable after all retries.
        Exception: For any other unrecoverable error.
    """
    start_time = time.time()

    # Input sanitization
    clean_query = sanitize_input(query)

    # Step 1: Route
    classification = classify_query(clean_query)
    model = settings.SIMPLE_MODEL if classification == "simple" else settings.COMPLEX_MODEL
    max_tokens = _MAX_TOKENS[classification]

    logger.info(
        "request_id=%s classification=%s model=%s", request_id, classification, model
    )

    # Step 2: Retrieve
    retrieved_chunks = retrieve(clean_query)
    retrieval_count = len(retrieved_chunks)

    # Step 3: Assemble prompt
    messages = assemble_prompt(clean_query, retrieved_chunks)

    # Step 4: Call Groq API (non-streaming)
    response = groq_client.generate(
        model=model,
        messages=messages,
        max_tokens=max_tokens,
        stream=False,
    )

    answer: str = response.choices[0].message.content or ""
    latency_ms = (time.time() - start_time) * 1000

    tokens_input: int = response.usage.prompt_tokens if response.usage else 0
    tokens_output: int = response.usage.completion_tokens if response.usage else 0

    # Step 5: Evaluate output
    evaluator_flags = evaluate_output(
        response_text=answer,
        retrieval_count=retrieval_count,
        retrieved_chunks=[c.to_dict() for c in retrieved_chunks],
    )

    # Step 6: Log
    log_query(
        request_id=request_id,
        query=clean_query,
        classification=classification,
        model_used=model,
        tokens_input=tokens_input,
        tokens_output=tokens_output,
        latency_ms=round(latency_ms, 2),
        retrieval_count=retrieval_count,
        retrieval_scores=[c.score for c in retrieved_chunks],
        evaluator_flags=evaluator_flags,
    )

    return {
        "request_id": request_id,
        "answer": answer,
        "sources": _sources_from_chunks(retrieved_chunks),
        "debug": _debug_payload(
            classification=classification,
            model=model,
            tokens_input=tokens_input,
            tokens_output=tokens_output,
            latency_ms=latency_ms,
            retrieval_count=retrieval_count,
            evaluator_flags=evaluator_flags,
        ),
    }


# ---------------------------------------------------------------------------
# Streaming pipeline
# ---------------------------------------------------------------------------


async def run_sse_pipeline(
    query: str,
    request_id: str,
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Run the full RAG query pipeline in SSE streaming mode.

    This is an async generator. The route handler wraps it in an
    EventSourceResponse — it must NOT be awaited.

    Each yielded dict has keys 'event' and 'data' (JSON string), matching
    the sse-starlette EventSourceResponse expected format:
      {"event": "token", "data": '{"token": "partial text"}'}
      {"event": "done",  "data": '<full metadata JSON>'}
      {"event": "error", "data": '<error JSON>'}

    Args:
        query: Raw user query string.
        request_id: UUID4 generated by the route handler.

    Yields:
        SSE event dicts.
    """
    start_time = time.time()

    # Input sanitization
    clean_query = sanitize_input(query)

    # Step 1: Route
    classification = classify_query(clean_query)
    model = settings.SIMPLE_MODEL if classification == "simple" else settings.COMPLEX_MODEL
    max_tokens = _MAX_TOKENS[classification]

    logger.info(
        "request_id=%s (stream) classification=%s model=%s",
        request_id,
        classification,
        model,
    )

    # Step 2: Retrieve
    retrieved_chunks = retrieve(clean_query)
    retrieval_count = len(retrieved_chunks)

    # Step 3: Assemble prompt
    messages = assemble_prompt(clean_query, retrieved_chunks)

    # Step 4: Call Groq API in streaming mode
    full_response = ""
    try:
        stream_response = groq_client.generate(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            stream=True,
        )

        for chunk in stream_response:
            delta = chunk.choices[0].delta
            if delta.content:
                token = delta.content
                full_response += token
                yield {
                    "event": "token",
                    "data": json.dumps({"token": token}),
                }

    except Exception as exc:
        latency_ms = (time.time() - start_time) * 1000
        logger.exception(
            "Streaming Groq error for request_id=%s after %.1f ms", request_id, latency_ms
        )
        log_query(
            request_id=request_id,
            query=clean_query,
            classification=classification,
            model_used=model,
            tokens_input=0,
            tokens_output=0,
            latency_ms=round(latency_ms, 2),
            retrieval_count=retrieval_count,
            retrieval_scores=[c.score for c in retrieved_chunks],
            evaluator_flags=[],
            error=str(exc),
        )
        yield {
            "event": "error",
            "data": json.dumps(
                {
                    "error": "The AI service is temporarily unavailable. Please try again.",
                    "request_id": request_id,
                    "status_code": 503,
                }
            ),
        }
        return

    # Stream completed — evaluate and log
    latency_ms = (time.time() - start_time) * 1000

    evaluator_flags = evaluate_output(
        response_text=full_response,
        retrieval_count=retrieval_count,
        retrieved_chunks=[c.to_dict() for c in retrieved_chunks],
    )

    log_query(
        request_id=request_id,
        query=clean_query,
        classification=classification,
        model_used=model,
        tokens_input=0,   # Streaming responses do not expose usage metadata
        tokens_output=0,
        latency_ms=round(latency_ms, 2),
        retrieval_count=retrieval_count,
        retrieval_scores=[c.score for c in retrieved_chunks],
        evaluator_flags=evaluator_flags,
    )

    # Emit 'done' event with full metadata
    yield {
        "event": "done",
        "data": json.dumps(
            {
                "request_id": request_id,
                "sources": _sources_from_chunks(retrieved_chunks),
                "debug": _debug_payload(
                    classification=classification,
                    model=model,
                    tokens_input=0,
                    tokens_output=0,
                    latency_ms=latency_ms,
                    retrieval_count=retrieval_count,
                    evaluator_flags=evaluator_flags,
                ),
            }
        ),
    }


# ---------------------------------------------------------------------------
# Synchronous convenience wrapper (used by tests)
# ---------------------------------------------------------------------------


def run_query(query: str, request_id: str, stream: bool = False) -> Dict[str, Any]:
    """
    Synchronous wrapper around run_query_pipeline.
    Used by unit/integration tests that cannot await coroutines directly.
    """
    return asyncio.run(run_query_pipeline(query, request_id))
